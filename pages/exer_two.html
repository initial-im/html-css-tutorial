<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]>      <html class="no-js"> <!--<![endif]-->
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>exercise #2</title>
    <meta name="description" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../styles/styles.css" />
  </head>
  <body>
    <nav><a href="../index.html">back to home</a></nav>
    <div class="news-body">
      <h2 class="title">
        Twitter’s new owner, Elon Musk, is feverishly promoting his “Twitter
        Files”
      </h2>
      <div class="small-text">Dec 20, 2030</div>
      <div class="small-text">written by Musk</div>

      <p class="paragraph">
        selected internal communications from the company, laboriously tweeted
        out by sympathetic amanuenses. But Musk’s obvious conviction that he has
        released some partisan kraken is mistaken — far from conspiracy or
        systemic abuse, the files are a valuable peek behind the curtain of
        moderation at scale, hinting at the Sisyphean labors undertaken by every
        social media platform. For a decade, companies like Twitter, YouTube,
        and Facebook have performed an elaborate dance to keep the details of
        their moderation processes equally out of reach of bad actors,
        regulators, and the press.
      </p>
      <p class="paragraph">
        To reveal too much would be to expose the processes to abuse by spammers
        and scammers (who indeed take advantage of every leaked or published
        detail), while to reveal too little leads to damaging reports and rumors
        as they lose control over the narrative. Meanwhile, they must be ready
        to justify and document their methods or risk censure and fines from
        government bodies.
      </p>
      <p class="paragraph">
        The result is that while everyone knows a little about how exactly these
        companies inspect, filter, and arrange the content posted on their
        platforms, it’s just enough to be sure that what we’re seeing is only
        the tip of the iceberg. Sometimes there are exposés of the methods we
        suspected — by-the-hour contractors clicking through violent and sexual
        imagery, an abhorrent but apparently necessary industry. Sometimes the
        companies overplay their hands, like repeated claims of how AI is
        revolutionizing moderation and subsequent reports that AI systems for
        this purpose are inscrutable and unreliable.
      </p>
      <div class="blockquote">
        "Everyone has the right to freedom of thought, conscience and religion;
        this right includes freedom to change his religion or belief, and
        freedom, either alone or in community with others and in public or
        private, to manifest his religion or belief in teaching, practice,
        worship and observance."
      </div>
      <p class="paragraph">
        ‘Behind the Screen’ illuminates the invisible, indispensable content
        moderation industry What almost never happens — generally companies
        don’t do this unless they’re forced to — is that the actual tools and
        processes of content moderation at scale are exposed with no filter. And
        that’s what Musk has done, perhaps to his own peril, but surely to the
        great interest of anyone who ever wondered what moderators actually do,
        say, and click as they make decisions that may affect millions.
      </p>

      <div class="list-elements">
        <ul>
          <li>twiter one</li>
          <li>twitter two</li>
        </ul>
        <ol>
          <li>amazon one</li>
          <li>amazon two</li>
        </ol>
      </div>
    </div>
  </body>
</html>